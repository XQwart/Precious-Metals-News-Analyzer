import json
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import feedparser
import logging
from typing import List, Dict
from dataclasses import dataclass

from filters import NewsPreFilter
from analyzer import OpenRouterAnalyzer
from extractor import ContentExtractor

logger = logging.getLogger(__name__)

@dataclass
class NewsItem:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
    title: str
    url: str
    source: str
    metals: List[str]
    published: str
    ai_summary: str
    relevance_score: float

class MetalsNewsParser:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –ø–∞—Ä—Å–µ—Ä–∞ —Å –º–æ–¥—É–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        self.pre_filter = NewsPreFilter()
        self.content_extractor = ContentExtractor()
        
        try:
            self.ai_analyzer = OpenRouterAnalyzer()
            if not self.ai_analyzer.test_connection():
                logger.warning("–ü—Ä–æ–±–ª–µ–º—ã —Å OpenRouter, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenRouter: {e}")
            self.ai_analyzer = None
        
        self.news_sources = {
            'rbc_economics': {
                'name': '–†–ë–ö –≠–∫–æ–Ω–æ–º–∏–∫–∞',
                'rss_urls': ['https://rssexport.rbc.ru/rbcnews/news/20/full.rss'],
                'base_url': 'https://www.rbc.ru'
            },
            'finam_news': {
                'name': '–§–∏–Ω–∞–º',
                'rss_urls': ['https://www.finam.ru/analysis/conews/rsspoint'],
                'base_url': 'https://www.finam.ru'
            },
            'investing_commodities': {
                'name': 'Investing –°—ã—Ä—å–µ',
                'rss_urls': ['https://ru.investing.com/rss/news.rss'],
                'base_url': 'https://ru.investing.com'
            },
            'vedomosti_economics': {
                'name': '–í–µ–¥–æ–º–æ—Å—Ç–∏ –≠–∫–æ–Ω–æ–º–∏–∫–∞',
                'rss_urls': ['https://www.vedomosti.ru/rss/economics'],
                'base_url': 'https://www.vedomosti.ru'
            }
        }
        
        self.stats = {
            'total_processed': 0,
            'pre_filtered_out': 0,
            'ai_analyzed': 0,
            'relevant_found': 0
        }
        
        logger.info(f"–ü–∞—Ä—Å–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å {len(self.news_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏")

    def parse_rss_feed(self, rss_url: str, source_name: str, max_age_hours: int = 24) -> List[NewsItem]:
        """–ü–∞—Ä—Å–∏—Ç RSS –ª–µ–Ω—Ç—É —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        news_items = []
        
        try:
            logger.info(f"üì° –ü–∞—Ä—Å–∏–Ω–≥ RSS: {rss_url}")
            
            feed = feedparser.parse(rss_url)
            
            if feed.bozo:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã —Å RSS: {feed.bozo_exception}")
            
            logger.info(f"üìä RSS —Å–æ–¥–µ—Ä–∂–∏—Ç {len(feed.entries)} –∑–∞–ø–∏—Å–µ–π")
            
            cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
            logger.info(f"‚è∞ –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ—Å–ª–µ: {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            filtered_by_time = 0
            
            for entry in feed.entries:
                try:
                    self.stats['total_processed'] += 1
                    
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            pub_date = datetime(*entry.published_parsed[:6])
                        except (ValueError, TypeError):
                            pub_date = datetime.now()
                    else:
                        pub_date = datetime.now()
                    
                    if pub_date < cutoff_time:
                        filtered_by_time += 1
                        continue
                    
                    title = getattr(entry, 'title', '')
                    summary = getattr(entry, 'summary', '')
                    url = getattr(entry, 'link', '')
                    
                    if not title or not url:
                        continue
                    
                    if summary:
                        summary = BeautifulSoup(summary, 'html.parser').get_text()
                    
                    should_process, preliminary_metals, reason = self.pre_filter.pre_filter_news(title, summary)
                    
                    if not should_process:
                        self.stats['pre_filtered_out'] += 1
                        continue
                    
                    logger.info(f"‚úÖ –ü—Ä–µ–¥—Ñ–∏–ª—å—Ç—Ä –ø—Ä–æ–π–¥–µ–Ω: {title[:50]}... (–º–µ—Ç–∞–ª–ª—ã: {', '.join(preliminary_metals)})")
                    
                    full_content = self.content_extractor.extract_article_content(url)
                    content_for_analysis = f"{title} {summary} {full_content}"
                    
                    self.stats['ai_analyzed'] += 1
                    logger.info(f"ü§ñ DeepSeek –∞–Ω–∞–ª–∏–∑: {title[:50]}...")
                    
                    if self.ai_analyzer:
                        analysis = self.ai_analyzer.analyze_news(title, content_for_analysis, preliminary_metals)
                    else:
                        analysis = {
                            "is_relevant": False, "metals": [], "summary": "", 
                            "score": 0.0, "reason": "AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
                        }
                    
                    if not analysis.get('is_relevant', False):
                        logger.info(f"‚ùå DeepSeek –æ—Ç–∫–ª–æ–Ω–∏–ª: {analysis.get('reason', '–Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ')}")
                        continue
                    
                    self.stats['relevant_found'] += 1
                    news_item = NewsItem(
                        title=title.strip(),
                        url=url,
                        source=source_name,
                        metals=analysis.get('metals', preliminary_metals),
                        published=pub_date.isoformat(),
                        ai_summary=analysis.get('summary', '')[:500],
                        relevance_score=analysis.get('score', 0.0)
                    )
                    
                    news_items.append(news_item)
                    logger.info(f"‚úÖ –ü–†–ò–ù–Ø–¢–û: {title[:60]}... (score: {analysis.get('score', 0):.2f})")
                    
                    time.sleep(1.5)
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {rss_url}: {e}")
        
        if 'filtered_by_time' not in locals():
            filtered_by_time = 0
        
        logger.info(f"üìà RSS —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –≤—Å–µ–≥–æ={len(feed.entries) if 'feed' in locals() else 0}, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –≤—Ä–µ–º–µ–Ω–∏={filtered_by_time}, –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö={len(news_items)}")
        
        return news_items

    def parse_all_sources(self, max_age_hours: int = 24) -> List[NewsItem]:
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
        all_news = []
        
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –º–æ–¥—É–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        for source_key, source_data in self.news_sources.items():
            logger.info(f"üì∞ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_data['name']}")
            source_news = []
            
            for rss_url in source_data['rss_urls']:
                try:
                    rss_news = self.parse_rss_feed(rss_url, source_data['name'], max_age_hours)
                    source_news.extend(rss_news)
                    time.sleep(2)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source_key}: {e}")
            
            all_news.extend(source_news)
            logger.info(f"üìä –ò—Å—Ç–æ—á–Ω–∏–∫ {source_data['name']}: {len(source_news)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        seen_urls = set()
        unique_news = []
        for news in all_news:
            if news.url not in seen_urls:
                seen_urls.add(news.url)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x.relevance_score, reverse=True)
        
        self.print_processing_stats()
        
        logger.info(f"üéØ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(unique_news)}")
        return unique_news

    def print_processing_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:")
        logger.info(f"   –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {self.stats['total_processed']}")
        logger.info(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø—Ä–µ–¥—Ñ–∏–ª—å—Ç—Ä–æ–º: {self.stats['pre_filtered_out']}")
        logger.info(f"   –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ AI –∞–Ω–∞–ª–∏–∑: {self.stats['ai_analyzed']}")
        logger.info(f"   –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {self.stats['relevant_found']}")
        
        if self.stats['ai_analyzed'] > 0:
            efficiency = (self.stats['relevant_found'] / self.stats['ai_analyzed']) * 100
            logger.info(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å AI –∞–Ω–∞–ª–∏–∑–∞: {efficiency:.1f}%")

    def save_to_json(self, news_items: List[NewsItem], filename: str = "metals_news.json") -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –≤ JSON —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞"""
        
        news_data = []
        for item in news_items:
            news_data.append({
                'title': item.title,
                'url': item.url,
                'source': item.source,
                'published': item.published,
                'ai_summary': item.ai_summary,
                'relevance_score': item.relevance_score
            })
        
        result = {
            'metadata': {
                'parsed_at': datetime.now().isoformat(),
                'total_news': len(news_items),
                'ai_provider': 'DeepSeek via OpenRouter.ai',
                'model': 'deepseek/deepseek-chat',
                'sources_count': len(set(item.source for item in news_items)),
                'metals_distribution': self.get_metals_stats(news_items),
                'average_relevance': sum(item.relevance_score for item in news_items) / len(news_items) if news_items else 0,
                'processing_stats': self.stats
            },
            'news': news_data
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, sort_keys=True)
        
        logger.info(f"üíæ –ù–æ–≤–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filename}")
        return filename

    def get_metals_stats(self, news_items: List[NewsItem]) -> Dict[str, int]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ—Ç–∞–ª–ª–∞–º"""
        stats = {}
        for item in news_items:
            for metal in item.metals:
                stats[metal] = stats.get(metal, 0) + 1
        return stats

    def print_summary(self, news_items: List[NewsItem]):
        """–í—ã–≤–æ–¥–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é —Å–≤–æ–¥–∫—É"""
        print(f"\n{'='*80}")
        print(f"üöÄ –ú–û–î–£–õ–¨–ù–´–ô DEEPSEEK –ü–ê–†–°–ï–† –î–†–ê–ì–û–¶–ï–ù–ù–´–• –ú–ï–¢–ê–õ–õ–û–í")
        print(f"{'='*80}")
        print(f"ü§ñ AI: DeepSeek —á–µ—Ä–µ–∑ OpenRouter.ai")
        print(f"üìä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news_items)}")
        
        if news_items:
            print(f"üéØ –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {sum(item.relevance_score for item in news_items) / len(news_items):.2f}")
        
        print(f"\nüìà –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨ –û–ë–†–ê–ë–û–¢–ö–ò:")
        print(f"   –í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {self.stats['total_processed']}")
        print(f"   –ü—Ä–µ–¥—Ñ–∏–ª—å—Ç—Ä –∏—Å–∫–ª—é—á–∏–ª: {self.stats['pre_filtered_out']}")
        print(f"   AI –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª: {self.stats['ai_analyzed']}")
        print(f"   –ò—Ç–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {self.stats['relevant_found']}")
        
        if self.stats['ai_analyzed'] > 0:
            efficiency = (self.stats['relevant_found'] / self.stats['ai_analyzed']) * 100
            api_savings = 100 - (self.stats['ai_analyzed'] / self.stats['total_processed']) * 100
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å AI: {efficiency:.1f}%")
            print(f"   –≠–∫–æ–Ω–æ–º–∏—è API: {api_savings:.1f}%")
        
        metals_stats = self.get_metals_stats(news_items)
        if metals_stats:
            print(f"\nü•á –ü–û –ú–ï–¢–ê–õ–õ–ê–ú:")
            for metal, count in metals_stats.items():
                print(f"   {metal.capitalize()}: {count}")
        
        sources_stats = {}
        for item in news_items:
            sources_stats[item.source] = sources_stats.get(item.source, 0) + 1
        
        if sources_stats:
            print(f"\nüì∞ –ü–û –ò–°–¢–û–ß–ù–ò–ö–ê–ú:")
            for source, count in sources_stats.items():
                print(f"   {source}: {count}")
        
        if news_items:
            print(f"\nüèÜ –¢–û–ü-3 –ù–û–í–û–°–¢–ò:")
            for i, item in enumerate(news_items[:3], 1):
                print(f"\n{i}. {item.title}")
                print(f"   üìä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {item.relevance_score:.2f} | üè¢ {item.source}")
                if item.ai_summary:
                    print(f"   ü§ñ {item.ai_summary}")
        
        print(f"\n{'='*80}\n")